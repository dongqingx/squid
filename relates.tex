\section{Related Works}
A significant amount of prior work has been done protecting privacy of network datasets.
We summarize them here and clarify our privacy goals in this paper. 

\textbf{Graph Anonymization.}~ Early works on privacy-preserving network data publishing mainly focus on developing anonymization techniques for deterministic graphs against specific types of de-anonymization attacks. Most of them leverage privacy models derived from $k$-anonymity to create $k$ identical neighborhoods, or $k$ identical degree nodes. Current methods for anonymizing ``graphs" can be classified into four main categories: (1) Clustering-based generation~\cite{Hay_Anonymizing_2007,Bhagat_Class_2009,hay2010resisting}; (2)~{\em Edge modification}~\cite{Liu_Towards_2008, Zhou_Preserving_2008, Wang2011, Wu_k_2010, Skarkala_Privacy_2012}, 
(3)~{\em Edge randomization}~\cite{Liu_Privacy_2009,Ying_Randomizing_2008, Ninggal_Utility_2015},
and~(4)~{\em Uncertainty semantic-based modifications} which add uncertainty to some edges and thus converting the graph to an uncertain version~\cite{Boldi_Injecting_2012, Nguyen_Anonymizing_2015}. 
The uncertainty semantic-based approaches transform the original determinitic graph into an uncertain one to be published. These techniques are known as the state-of-art for their excellent privacy-utiltiy tradeoff brought by the fine-grained pertubation leveraging the uncertain semantics. As ever mentioned, these techniques are tailored to determinitic graphs (unweighted \& weighted). In this work, we further explore the privacy preserving problem in uncertain graph publication. 


\textbf{Diffiential Privacy.}~ The recent research on applying differential privacy to network data roughly falls into two directions. The first direction aims to release certain differentially private data mining results, such as degree distributions, sub-graph counts and frequent graph patterns. Such methods that release only query results require tracking tracking the results: early uses of the data can effect the quality of later uses, thus no new queries can be permitted on the data.  
The second direction aims to publish a sanitized graph. Most research in this direction projects an input graph to dK-series and ensures differential privacy on dK-series statistics. These private statistics are then either fed into generators or MCMC process to generate a fit synthetic graphs. While, current techniques are still inadequate to provide desirable data utility for many graph mining tasks. 



\textbf{Our Goals:Privacy Model \& Data Model}~
$\epsilon$-differential privacy does relate to individual identifiability and provide strong privacy guarntee without making any assumption of privacy risks. While, the privacy parameter $\epsilon$ does not have such a clear relation. There is no clear way to set a general policy for a value $\epsilon$ that provides sufficient privacy. In constrast to $\epsilon$-differential privacy, syntactic privacy model can generally be defined and understood based on the data schema; parameters have a clear privacy meaning that can be understood independent of the actual data and have a clear relationship to the legal concept of individual identifiability of data~\cite{}.  

Our general approach is to produce synthetic graphs by adding controlled perturbations to the structure of the original \emph{uncertain graph}. The generic approach can provide protection against identity disclosure and link disclosure. The choice directly impacts how much structural noise must be introduced to obtain a given level of privacy guarantees. In this paper, we chose to target against node re-identification problem for {\emph uncertain graph} releasing because . 