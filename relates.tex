\section{Related Works}
A significant amount of prior work has been done protecting privacy of network datasets.
We summarize them here and clarify our privacy goals in this paper. 

\textbf{Graph Anonymization.}~ Early works on privacy-preserving network publishing mainly focus on developing anonymization techniques for deterministic graphs. The goal is to \emph{publish} the data in anonymized manner without making any assumptionsof the type of analysis and queries that will be executed on it. Once the data is published, it is available for any type of analysis. 
Most of them leverage privacy models derived from $k$-anonymity to create $k$ identical neighborhoods, or $k$ identical degree nodes. Current methods for anonymizing ``graphs" can be classified into four main categories: (1) Clustering-based generation~\cite{Hay_Anonymizing_2007,Bhagat_Class_2009,hay2010resisting}; (2)~{\em Edge modification}~\cite{Liu_Towards_2008, Zhou_Preserving_2008, Wang2011, Wu_k_2010, Skarkala_Privacy_2012}, 
(3)~{\em Edge randomization}~\cite{Liu_Privacy_2009,Ying_Randomizing_2008, Ninggal_Utility_2015},
and~(4)~{\em Uncertainty semantic-based modifications} which add uncertainty to some edges and thus converting the graph to an uncertain version~\cite{Boldi_Injecting_2012, Nguyen_Anonymizing_2015}. 
The uncertainty semantic-based approaches transform the original deterministic graph into an uncertain one to be published. These techniques are known as the state-of-art for their excellent privacy-utility tradeoff brought by the fine-grained perturbation leveraging the uncertain semantics. As ever mentioned, these techniques are tailored to \emph{deterministic} graphs (unweighted \& weighted). In this work, we further explore the privacy preserving problem in uncertain graph publication. 


\textbf{Diffiential Privacy.}~ The recent research on applying differential privacy to network data roughly falls into two directions. The first direction aims to release certain differentially private data mining results, such as degree distributions, sub-graph counts and frequent graph patterns. Such methods that release only query results require tracking the results: early uses of the data can affect the quality of later uses, thus no new queries can be permitted on the data.  
The second direction aims to publish a sanitized graph. Most research in this direction projects an input graph to dK-series and ensures differential privacy on dK-series statistics. These private statistics are then either fed into generators or MCMC process to generate a fit synthetic graphs. While current techniques are still inadequate to provide desirable data utility for many graph mining tasks. 



\textbf{Our Goals:Privacy Model \& Data Model}~
$\epsilon$-differential privacy does relate to individual identifiability and provides strong privacy guarantee without making any assumption of privacy risks. While, there is no clear way to set a general policy for a value $\epsilon$ that provides sufficient privacy~\cite{lee2011}. In contrast to $\epsilon$-differential privacy, syntactic privacy model can generally be defined and understood based on the data schema; parameters have a clear privacy meaning that can be understood independent of the actual data and have a clear relationship to the legal concept of individual identifiability of data.  

Our general approach is to produce synthetic graphs by adding controlled perturbations to the structure of the original \emph{uncertain graph}. The generic approach can provide protection against identity disclosure and link disclosure. The choice directly impacts how much structural noise must be introduced to obtain a given level of privacy guarantees. In this paper, we chose to target against node re-identification problem for {\emph uncertain graph} releasing.  